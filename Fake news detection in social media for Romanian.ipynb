{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dangerous-cheat",
   "metadata": {},
   "source": [
    "# Importing the necessary modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "medium-martial",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utilitaries\n",
    "from csv import reader\n",
    "import re\n",
    "import pandas as pd\n",
    "import zeep\n",
    "# Text manipulation\n",
    "\n",
    "# Data preparation\n",
    "import nltk\n",
    "from nltk.corpus import stopwords as stp\n",
    "import nltk.stem.snowball as snowball\n",
    "# Machine learning tools\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "# Machine learning classificators\n",
    "from sklearn.linear_model import PassiveAggressiveClassifier\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "# Machine learning model metrics\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import log_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "owned-operator",
   "metadata": {},
   "source": [
    "# Reading the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "supposed-species",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('corpus.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "hidden-fourth",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>title</th>\n",
       "      <th>translation</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>928</th>\n",
       "      <td>TVR</td>\n",
       "      <td>Rezultate parțiale BEC, după centralizarea a 9...</td>\n",
       "      <td>PSD a obținut la alegerile parlamentare  29,64...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>920</th>\n",
       "      <td>TVR</td>\n",
       "      <td>Marcel Ciolacu: Voi avea o discuție cu colegii...</td>\n",
       "      <td>Președintele PSD, Marcel Ciolacu, a declarat c...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>430</th>\n",
       "      <td>Ionut</td>\n",
       "      <td>Cel mai norocos oltean. A găsit o sarma cu pat...</td>\n",
       "      <td>Chiar dacă a găsit sarmaua cu patru foi, oltea...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>700</th>\n",
       "      <td>TVR</td>\n",
       "      <td>Bugetul, dezbătut intens în comisiile de speci...</td>\n",
       "      <td>ACTUALIZARE Bugetul Casei Naţionale de Asigură...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>342</th>\n",
       "      <td>Eftimie</td>\n",
       "      <td>Fum alb la negocieri. Ludovic Orban fumează ia...</td>\n",
       "      <td>„Doar un fum să mai trag”, a declarat liderul ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      author                                              title  \\\n",
       "928      TVR  Rezultate parțiale BEC, după centralizarea a 9...   \n",
       "920      TVR  Marcel Ciolacu: Voi avea o discuție cu colegii...   \n",
       "430    Ionut  Cel mai norocos oltean. A găsit o sarma cu pat...   \n",
       "700      TVR  Bugetul, dezbătut intens în comisiile de speci...   \n",
       "342  Eftimie  Fum alb la negocieri. Ludovic Orban fumează ia...   \n",
       "\n",
       "                                           translation  label  \n",
       "928  PSD a obținut la alegerile parlamentare  29,64...      1  \n",
       "920  Președintele PSD, Marcel Ciolacu, a declarat c...      1  \n",
       "430  Chiar dacă a găsit sarmaua cu patru foi, oltea...      0  \n",
       "700  ACTUALIZARE Bugetul Casei Naţionale de Asigură...      1  \n",
       "342  „Doar un fum să mai trag”, a declarat liderul ...      0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "touched-sacramento",
   "metadata": {},
   "source": [
    "# Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "consolidated-samuel",
   "metadata": {},
   "source": [
    "### Lemmatization\n",
    "_Source: http://nlptools.info.uaic.ro/WebPosRo/_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "stretch-cathedral",
   "metadata": {},
   "outputs": [],
   "source": [
    "wsdl = 'http://nlptools.info.uaic.ro/WebPosRo/PosTaggerRoWS?wsdl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "blond-alabama",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = zeep.Client(wsdl=wsdl)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "frozen-owner",
   "metadata": {},
   "source": [
    "#### Transforming each article using the POS tagger,tokenizing and filtering the stopwords at the same time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "suburban-devon",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatized_articles_xml=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "worthy-count",
   "metadata": {},
   "outputs": [],
   "source": [
    "for article in df['translation']:\n",
    "     lemmatized_articles_xml.append(client.service.parseSentence_XML(article))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "stopped-broad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Saving the articles to save execution time\n",
    "# for article in lemmatized_articles_xml:\n",
    "#     with open('lemmatized_articles','a+') as f:\n",
    "#         f.write(article)\n",
    "#         f.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "conventional-ocean",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = stp.words('romanian')\n",
    "lemmatized_articles_clean=[]\n",
    "for article in lemmatized_articles_xml:\n",
    "    article = article.split('\\n')\n",
    "    lemmas = []\n",
    "    for line in article:\n",
    "        found = re.search('.*LEMMA=\"([A-Za-zăâîșț1-9]*)\".*', line)\n",
    "        if found:\n",
    "            word = found.group(1)\n",
    "            if word not in stopwords:\n",
    "                lemmas.append(word)\n",
    "    lemmatized_articles_clean.append(lemmas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "asian-inquiry",
   "metadata": {},
   "source": [
    "#### Adding the lemmatized articles to the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "polished-dating",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['lemmatized_articles'] = lemmatized_articles_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "committed-literacy",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>title</th>\n",
       "      <th>translation</th>\n",
       "      <th>label</th>\n",
       "      <th>lemmatized_articles</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>838</th>\n",
       "      <td>TVR</td>\n",
       "      <td>Cum își petrec politicienii Crăciunul</td>\n",
       "      <td>Masa trebuie să fie plină de bucate speciale î...</td>\n",
       "      <td>1</td>\n",
       "      <td>[masă, trebui, plin, bucată, special, Cristina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>542</th>\n",
       "      <td>Vasile</td>\n",
       "      <td>Testul COVID făcut de naționala de fotbal arat...</td>\n",
       "      <td>„COVID n-am găsit, dar până la urmă, mai conte...</td>\n",
       "      <td>0</td>\n",
       "      <td>[COVID, găsi, conta, adăuga, medic, testa, spo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>742</th>\n",
       "      <td>TVR</td>\n",
       "      <td>Diana Șoșoacă, fără sprijinul partidului AUR</td>\n",
       "      <td>DAN TANASĂ, deputat AUR: \"M-a deranjat acel ep...</td>\n",
       "      <td>1</td>\n",
       "      <td>[dan, tanasă, deputat, aur, deranja, episod, d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>Calin</td>\n",
       "      <td>Ce ghiolban! O specie de țânțar râgâie după ce...</td>\n",
       "      <td>”Nu sunt adeptul soluțiilor extreme, dar aceas...</td>\n",
       "      <td>0</td>\n",
       "      <td>[adept, soluție, extrem, specie, țânțar, trebu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>751</th>\n",
       "      <td>TVR</td>\n",
       "      <td>Orban: Nu s-a luat nicio decizie legată de spo...</td>\n",
       "      <td>El a adăugat că studenţii vor avea în continua...</td>\n",
       "      <td>1</td>\n",
       "      <td>[adăuga, student, vrea, gratuitate, călătorie,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     author                                              title  \\\n",
       "838     TVR              Cum își petrec politicienii Crăciunul   \n",
       "542  Vasile  Testul COVID făcut de naționala de fotbal arat...   \n",
       "742     TVR       Diana Șoșoacă, fără sprijinul partidului AUR   \n",
       "141   Calin  Ce ghiolban! O specie de țânțar râgâie după ce...   \n",
       "751     TVR  Orban: Nu s-a luat nicio decizie legată de spo...   \n",
       "\n",
       "                                           translation  label  \\\n",
       "838  Masa trebuie să fie plină de bucate speciale î...      1   \n",
       "542  „COVID n-am găsit, dar până la urmă, mai conte...      0   \n",
       "742  DAN TANASĂ, deputat AUR: \"M-a deranjat acel ep...      1   \n",
       "141  ”Nu sunt adeptul soluțiilor extreme, dar aceas...      0   \n",
       "751  El a adăugat că studenţii vor avea în continua...      1   \n",
       "\n",
       "                                   lemmatized_articles  \n",
       "838  [masă, trebui, plin, bucată, special, Cristina...  \n",
       "542  [COVID, găsi, conta, adăuga, medic, testa, spo...  \n",
       "742  [dan, tanasă, deputat, aur, deranja, episod, d...  \n",
       "141  [adept, soluție, extrem, specie, țânțar, trebu...  \n",
       "751  [adăuga, student, vrea, gratuitate, călătorie,...  "
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sorted-composition",
   "metadata": {},
   "source": [
    "#### Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "executive-functionality",
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmed_articles = []\n",
    "stemmer = snowball.SnowballStemmer('romanian')\n",
    "\n",
    "for article in df['lemmatized_articles']:\n",
    "    stemmed_words =[]\n",
    "    for word in article:\n",
    "        stemmed_words.append(stemmer.stem(word))\n",
    "        # Appending the string form of the stemmed array because it will\n",
    "        # be needed in the next step\n",
    "    stemmed_articles.append(' '.join(stemmed_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "victorian-entrance",
   "metadata": {},
   "source": [
    "#### Adding the stemmed articles to the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "hindu-battlefield",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['stemmed_articles']=stemmed_articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "exposed-cambridge",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>title</th>\n",
       "      <th>translation</th>\n",
       "      <th>label</th>\n",
       "      <th>lemmatized_articles</th>\n",
       "      <th>stemmed_articles</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>587</th>\n",
       "      <td>Calin</td>\n",
       "      <td>România a decis să nu se prezinte la meciul cu...</td>\n",
       "      <td>Vestea a fost primită cu optimism de jucătorii...</td>\n",
       "      <td>0</td>\n",
       "      <td>[veste, primit, optimism, jucător, lot, națion...</td>\n",
       "      <td>vest primit optimist jucat lot național grij v...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>398</th>\n",
       "      <td>Vasile</td>\n",
       "      <td>Validarea lui Ciucu ca primar, blocată în inst...</td>\n",
       "      <td>„Specialiştii noştri atrag atenţia că numele d...</td>\n",
       "      <td>0</td>\n",
       "      <td>[specialist, atrage, atenție, nume, domn, ciuc...</td>\n",
       "      <td>specialist atrag atenț num domn ciucu asemăn b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>345</th>\n",
       "      <td>Calin</td>\n",
       "      <td>Nea Costel s-a consultat cu liderii de birt și...</td>\n",
       "      <td>„Am un singur mesaj pentru Orban: dă-te-n p… m...</td>\n",
       "      <td>0</td>\n",
       "      <td>[singur, mesaj, orban, puncte, transmite, nea,...</td>\n",
       "      <td>singur mesaj orban punct transmit nea costel ț...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>404</th>\n",
       "      <td>Vasile</td>\n",
       "      <td>Viorica Dăncilă îl acuză pe Iohannis că a inve...</td>\n",
       "      <td>„Nu doar cuvinte ca giroscop sau funambulesc s...</td>\n",
       "      <td>0</td>\n",
       "      <td>[cuvânt, giroscop, funambulesc, inventat, Ioha...</td>\n",
       "      <td>cuvânt giroscop funambul invent iohannis și ga...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169</th>\n",
       "      <td>Eftimie</td>\n",
       "      <td>Coca-Cola salvează industria ţiţeiului dublând...</td>\n",
       "      <td>Pe lângă cola, și alte băuturi din gamă vor be...</td>\n",
       "      <td>0</td>\n",
       "      <td>[cola, și, băutură, gamă, vrea, beneficia, pet...</td>\n",
       "      <td>col și băut gam vre benefic petrol gust putern...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      author                                              title  \\\n",
       "587    Calin  România a decis să nu se prezinte la meciul cu...   \n",
       "398   Vasile  Validarea lui Ciucu ca primar, blocată în inst...   \n",
       "345    Calin  Nea Costel s-a consultat cu liderii de birt și...   \n",
       "404   Vasile  Viorica Dăncilă îl acuză pe Iohannis că a inve...   \n",
       "169  Eftimie  Coca-Cola salvează industria ţiţeiului dublând...   \n",
       "\n",
       "                                           translation  label  \\\n",
       "587  Vestea a fost primită cu optimism de jucătorii...      0   \n",
       "398  „Specialiştii noştri atrag atenţia că numele d...      0   \n",
       "345  „Am un singur mesaj pentru Orban: dă-te-n p… m...      0   \n",
       "404  „Nu doar cuvinte ca giroscop sau funambulesc s...      0   \n",
       "169  Pe lângă cola, și alte băuturi din gamă vor be...      0   \n",
       "\n",
       "                                   lemmatized_articles  \\\n",
       "587  [veste, primit, optimism, jucător, lot, națion...   \n",
       "398  [specialist, atrage, atenție, nume, domn, ciuc...   \n",
       "345  [singur, mesaj, orban, puncte, transmite, nea,...   \n",
       "404  [cuvânt, giroscop, funambulesc, inventat, Ioha...   \n",
       "169  [cola, și, băutură, gamă, vrea, beneficia, pet...   \n",
       "\n",
       "                                      stemmed_articles  \n",
       "587  vest primit optimist jucat lot național grij v...  \n",
       "398  specialist atrag atenț num domn ciucu asemăn b...  \n",
       "345  singur mesaj orban punct transmit nea costel ț...  \n",
       "404  cuvânt giroscop funambul invent iohannis și ga...  \n",
       "169  col și băut gam vre benefic petrol gust putern...  "
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tender-example",
   "metadata": {},
   "source": [
    "# Preparation for the Machine Learning training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acting-parish",
   "metadata": {},
   "source": [
    "#### Extracting the article labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "static-forth",
   "metadata": {},
   "outputs": [],
   "source": [
    "label = df.label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "thirty-fantasy",
   "metadata": {},
   "source": [
    "#### Splitting the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "dying-avenue",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_test,y_train,y_test=train_test_split(df['stemmed_articles'], label, test_size=0.2,shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eligible-gallery",
   "metadata": {},
   "source": [
    "#### Preparing the tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "lesser-oregon",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aquatic-sitting",
   "metadata": {},
   "source": [
    "# Trying different Machine Learning models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "handy-board",
   "metadata": {},
   "source": [
    "## Bayes Naive"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "minimal-continent",
   "metadata": {},
   "source": [
    "### BN with TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "miniature-cooper",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = tokenizer.texts_to_matrix(X_train,mode='tfidf')\n",
    "X_test = tokenizer.texts_to_matrix(X_test,mode='tfidf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "ideal-shopping",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### _After each transformation its necessary to recompute X_train and X_test again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "renewable-voice",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GaussianNB()"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb_classifier = GaussianNB()\n",
    "nb_classifier.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "informed-jenny",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = nb_classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "sixth-paradise",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 81.37%\n",
      "Recall: 61.7%\n",
      "loss: 6.43\n"
     ]
    }
   ],
   "source": [
    "accuracy=accuracy_score(y_test,y_pred)\n",
    "loss = log_loss(y_test,y_pred)\n",
    "recall = recall_score(y_test,y_pred)\n",
    "print(f'Accuracy: {round(accuracy*100,2)}%\\nRecall: {round(recall*100,2)}%\\nloss: {round(loss,2)}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "directed-married",
   "metadata": {},
   "source": [
    "### BN with Count-Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "native-queen",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_test,y_train,y_test=train_test_split(df['stemmed_articles'], label, test_size=0.2,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "particular-unknown",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = tokenizer.texts_to_matrix(X_train,mode='count')\n",
    "X_test = tokenizer.texts_to_matrix(X_test,mode='count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "complicated-sugar",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GaussianNB()"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb_classifier = GaussianNB()\n",
    "nb_classifier.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "improved-distance",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = nb_classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "fleet-andrew",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 83.33%\n",
      "Recall: 57.5%\n",
      "loss: 5.76\n"
     ]
    }
   ],
   "source": [
    "accuracy=accuracy_score(y_test,y_pred)\n",
    "loss = log_loss(y_test,y_pred)\n",
    "recall = recall_score(y_test,y_pred)\n",
    "print(f'Accuracy: {round(accuracy*100,2)}%\\nRecall: {round(recall*100,2)}%\\nloss: {round(loss,2)}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "advance-trading",
   "metadata": {},
   "source": [
    "## Logistic Regression "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "permanent-actress",
   "metadata": {},
   "source": [
    "### LR with TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "impaired-species",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_test,y_train,y_test=train_test_split(df['stemmed_articles'], label, test_size=0.2,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "correct-capital",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = tokenizer.texts_to_matrix(X_train,mode='tfidf')\n",
    "X_test = tokenizer.texts_to_matrix(X_test,mode='tfidf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "frozen-kidney",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_classifier = LogisticRegression()\n",
    "lr_classifier.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "funded-albert",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = nb_classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "promotional-conviction",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 96.08%\n",
      "Recall: 89.47%\n",
      "loss: 1.35\n"
     ]
    }
   ],
   "source": [
    "accuracy=accuracy_score(y_test,y_pred)\n",
    "loss = log_loss(y_test,y_pred)\n",
    "recall = recall_score(y_test,y_pred)\n",
    "print(f'Accuracy: {round(accuracy*100,2)}%\\nRecall: {round(recall*100,2)}%\\nloss: {round(loss,2)}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "informative-default",
   "metadata": {},
   "source": [
    "### LR with Count-Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "unlimited-romania",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_test,y_train,y_test=train_test_split(df['stemmed_articles'], label, test_size=0.2,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "proof-packaging",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = tokenizer.texts_to_matrix(X_train,mode='count')\n",
    "X_test = tokenizer.texts_to_matrix(X_test,mode='count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "descending-swift",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_classifier = LogisticRegression()\n",
    "lr_classifier.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "greatest-discount",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = lr_classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "least-subscription",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 90.2%\n",
      "Recall: 89.74%\n",
      "loss: 3.39\n"
     ]
    }
   ],
   "source": [
    "accuracy=accuracy_score(y_test,y_pred)\n",
    "loss = log_loss(y_test,y_pred)\n",
    "recall = recall_score(y_test,y_pred)\n",
    "print(f'Accuracy: {round(accuracy*100,2)}%\\nRecall: {round(recall*100,2)}%\\nloss: {round(loss,2)}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "early-midnight",
   "metadata": {},
   "source": [
    "## SGD "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "willing-kenya",
   "metadata": {},
   "source": [
    "### SGD with TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "stopped-commissioner",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_test,y_train,y_test=train_test_split(df['stemmed_articles'], label, test_size=0.2,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "undefined-registrar",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = tokenizer.texts_to_matrix(X_train,mode='tfidf')\n",
    "X_test = tokenizer.texts_to_matrix(X_test,mode='tfidf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "virtual-tobacco",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SGDClassifier()"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sgd_classifier = SGDClassifier()\n",
    "sgd_classifier.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "residential-profession",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = nb_classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "paperback-angel",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 98.04%\n",
      "Recall: 94.94%\n",
      "loss: 0.68\n"
     ]
    }
   ],
   "source": [
    "accuracy=accuracy_score(y_test,y_pred)\n",
    "loss = log_loss(y_test,y_pred)\n",
    "recall = recall_score(y_test,y_pred)\n",
    "print(f'Accuracy: {round(accuracy*100,2)}%\\nRecall: {round(recall*100,2)}%\\nloss: {round(loss,2)}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "effective-pleasure",
   "metadata": {},
   "source": [
    "### SGD with Count-Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "numerous-building",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_test,y_train,y_test=train_test_split(df['stemmed_articles'], label, test_size=0.2,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "covered-outside",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = tokenizer.texts_to_matrix(X_train,mode='count')\n",
    "X_test = tokenizer.texts_to_matrix(X_test,mode='count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "decimal-associate",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SGDClassifier()"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sgd_classifier = SGDClassifier()\n",
    "sgd_classifier.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "precious-entity",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = lr_classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "pleasant-privacy",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 96.57%\n",
      "Recall: 98.75%\n",
      "loss: 1.19\n"
     ]
    }
   ],
   "source": [
    "accuracy=accuracy_score(y_test,y_pred)\n",
    "loss = log_loss(y_test,y_pred)\n",
    "recall = recall_score(y_test,y_pred)\n",
    "print(f'Accuracy: {round(accuracy*100,2)}%\\nRecall: {round(recall*100,2)}%\\nloss: {round(loss,2)}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "clean-investing",
   "metadata": {},
   "source": [
    "## Passive Agressive Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "federal-bleeding",
   "metadata": {},
   "source": [
    "### PAC with TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "flush-testimony",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_test,y_train,y_test=train_test_split(df['stemmed_articles'], label, test_size=0.2,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "noted-funds",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = tokenizer.texts_to_matrix(X_train,mode='tfidf')\n",
    "X_test = tokenizer.texts_to_matrix(X_test,mode='tfidf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "fiscal-senator",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PassiveAggressiveClassifier()"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pac_classifier = PassiveAggressiveClassifier()\n",
    "pac_classifier.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "dangerous-composition",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = nb_classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "hawaiian-hybrid",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 96.57%\n",
      "Recall: 91.46%\n",
      "loss: 1.19\n"
     ]
    }
   ],
   "source": [
    "accuracy=accuracy_score(y_test,y_pred)\n",
    "loss = log_loss(y_test,y_pred)\n",
    "recall = recall_score(y_test,y_pred)\n",
    "print(f'Accuracy: {round(accuracy*100,2)}%\\nRecall: {round(recall*100,2)}%\\nloss: {round(loss,2)}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "brilliant-theta",
   "metadata": {},
   "source": [
    "### PAC with Count-Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "commercial-bahrain",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_test,y_train,y_test=train_test_split(df['stemmed_articles'], label, test_size=0.2,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "characteristic-russian",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = tokenizer.texts_to_matrix(X_train,mode='count')\n",
    "X_test = tokenizer.texts_to_matrix(X_test,mode='count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "obvious-swaziland",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SGDClassifier()"
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pac_classifier = SGDClassifier()\n",
    "pac_classifier.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "id": "fabulous-cannon",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = lr_classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "id": "banned-service",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 98.04%\n",
      "Recall: 96.55%\n",
      "loss: 0.68\n"
     ]
    }
   ],
   "source": [
    "accuracy=accuracy_score(y_test,y_pred)\n",
    "loss = log_loss(y_test,y_pred)\n",
    "recall = recall_score(y_test,y_pred)\n",
    "print(f'Accuracy: {round(accuracy*100,2)}%\\nRecall: {round(recall*100,2)}%\\nloss: {round(loss,2)}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
